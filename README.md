<h1>Chat with PDF</h1>

<p>LLM app with RAG to chat with PDF files using Llama 3.2 running locally. The app uses Retrieval Augmented Generation (RAG) to provide accurate answers to questions based on the content of the uploaded PDF.</p>

<h2>Features</h2>

<ul>
  <li>Upload a PDF document</li>
  <li>Ask questions about the content of the PDF</li>
  <li>Get accurate answers using RAG and the Llama 3.2</li>
</ul>

<h2>How to get Started?</h2>

<ol>
  <li>Clone the GitHub repository</li>
</ol>

<pre><code>https://github.com/Sumanth077/chat_with_pdf.git</code></pre>

<ol start="2">
  <li>Install the required dependencies</li>
</ol>

<pre><code>pip install -r requirements.txt</code></pre>

<ol start="3">
  <li>Run the Reflex App</li>
</ol>

<pre><code>reflex run</code></pre>

<h2>Application Demo</h2>

https://github.com/Sumanth077/Llama2-Chat/blob/main/chat_with_pdf_locally.mp4
